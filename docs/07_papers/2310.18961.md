---
title: "AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection"
layout: default
parent: Papers
---


# AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection

> **Accept info**: ICLR 2024  
> **Authors**: Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen  
> **Affiliation**: Zhejiang University, Singapore Management University, Harvard University  
> **Links**: [arXiv](https://arxiv.org/abs/2310.18961), [OpenReview](https://openreview.net/forum?id=buC4E91xZE), [GitHub](https://github.com/zqhang/AnomalyCLIP)  
> **Task**: zero-shot anomaly detection (ZSAD)  
> **TLDR**: learning object-agnostic prompt is key to ZSAD.  
 

## 1. Intuition & Motivation

Zero-shot anomaly detection (ZSAD): detect anomalies without any training sample in a target dataset  

For ZSAD, previous works exploit the CLIP's generalizability by using specialized object-aware text prompts.  
For example, WinCLIP use a large number of hand-crafted text prompts for ZSAD.  

<img width="100%" alt="Figure1_comparison" src="https://github.com/user-attachments/assets/9c11fe77-40ab-4125-9776-e0735ef93c0b">

However, the current prompting approaches fail to capture the abnormality as shown in `Figure 1 (c), (d), (e)`.  
Why?  
Since CLIP is pre-trained to align with the class semantics of the foreground objects, CLIP with object-aware prompt will also focus on foreground semantics rather than abnormality/normality in the images.  
Natural question may follow: **do we need foreground object semantics in ZSAD?**  

Even though the foreground object semantics can be completely different, anomaly patterns remain quite similar.  
Thus, it is natural to learn object-agnostic text prompts, not object-aware text prompts.  

Then how can we make CLIP to not focus on object semantics?  
The most simple approach would be excluding the object semantics from text prompt templates.  
(ex. `A photo of a damaged [class]` â†’ `A photo of a damaged [object]`)  



## 2. AnomalyCLIP: object-agnostic prompt learning
<img width="100%" alt="Figure2_overview" src="https://github.com/user-attachments/assets/dfd1d459-1f0b-4561-a419-5a507fad56b9">


### 2.1. Approach overview

- Core method: object-agnostic text prompt templates  
- Training objective: cross-entropy, focal, dice loss  
  image-level (global): cross-entropy loss  
  pixel-level (local): focal loss, dice loss
- Other methods: text prompt tuning, DPAM  
  text prompt tuning: learnable token embeddings in text encoder  
  DPAM: replace Q-K self-attention with diagonally prominent attention (ex. V-V self-attention)  
- **Only train text prompt templates and token embeddings**  


### 2.2. Object-agnostic text prompt design

- Object-aware text prompt templates (previous works)  
  text embeddings of normality: `[V_1][V_2]...[V_E][cls]`  
  text embeddings of abnormality: `[W_1][W_2]...[W_E][damaged][cls]`  

- **Object-agnostic text prompt templates (our work)**  
  text embeddings of normality ($$g_n$$): `[V_1][V_2]...[V_E][object]`  
  text embeddings of abnormality ($$g_a$$): `[W_1][W_2]...[W_E][damaged][object]`  


### 2.3. Refinement of the textual space

Use text prompt tuning to refine the original textual space of CLIP.  
Following the previous works ([VPT](https://arxiv.org/abs/2203.12119), [MaPLe](https://arxiv.org/abs/2210.03117)), add additional learnable token embeddings into its text encoder.  


### 2.4. Refinement of the local visual space
<img width="100%" alt="Figure3_attention_vis" src="https://github.com/user-attachments/assets/3475246d-6165-4cf3-a43a-388a4413fafe">

Attention map in the visual encoder focuses on the specific tokens.  
(ex. `Figure 1 (b)`)  
These tokens disrupt the local visual semantics, hindering the effective learning of the fine-grained abnormality.  

Authors empirically find that a Diagonally Prominent Attention Map (DPAM) helps reduce the disturbance from other tokens, leading to improved local visual semantics.  
Thus, replace the original Q-K attention in the visual encoder with diagonally prominent attention.  
(ex. Q-Q, K-K, V-V self-attention)  


### 2.5. Training and inference

<details><summary>Training</summary>
<img width="100%" alt="Equation2_loss" src="https://github.com/user-attachments/assets/b55dd1ed-c105-4f3b-a520-1ae2eaa92a5e">

> Objective: glocal loss (global + local)  
> Global loss: cross-entropy loss  
> Local loss: focal loss, dice loss  
> 
> Integrate intermediate layers to provide more local visual details.  

</details>


<details><summary>Inference</summary>

- Image-level anomaly score: $$P(g_a, f_i)$$  
  $$P(\cdot, \cdot)$$: similarity score used in CLIP  
  $$g_a$$: learned abnormality text embeddings  
  $$f_i$$: global visual embedding  

<img width="100%" alt="Inference_mask" src="https://github.com/user-attachments/assets/86e7f723-bd88-4971-a302-3f67143b8575">

- Pixel-level prediction: merge the segmentation of all selected intermediate layers with interpolation and smoothing operation  

</details>



## 3. Experiments
### 3.1. Experiment setup

- Datasets  
  17 benchmark dataset (7 industrial, 10 medical)  
- Metrics  
  Area Under the Receiver Operating Characteristic Curve (AUROC)  
  Average Precision (AP) for anomaly detection  
  AUPRO for anomaly segmentation  
- Implementation details  
  use CLIP ViT-L/14@336  
  replace Q-K self-attention with V-V self-attention  
  fine-tune on MVTec AD test, evaluate on other datasets  
  (for MVTec AD, fine-tune with VisA test)  


### 3.2. Main results

<details><summary>Table 1</summary>
<img width="100%" alt="Table1_result" src="https://github.com/user-attachments/assets/9a3803df-111e-4004-8f33-92a0349a7070">

> `Table 1`  
> AnomalyCLIP achieves superior ZSAD performance across the datasets.  

</details>


<details><summary>Table 2</summary>
<img width="100%" alt="Table2_result" src="https://github.com/user-attachments/assets/1e1a6674-18f8-47cb-9863-3bba93a58c3b">

> `Table 2`  
> AnomalyCLIP obtain promising ZSAD performance on various medical image datasets, even though they are tuned using a defect detection dataset.  

</details>


<details><summary>Table 3</summary>
<img width="55%" alt="Table3_result" src="https://github.com/user-attachments/assets/22429ea1-a993-4ca8-bc9e-402a0bb54a89">

> `Table 3`  
> Even if AnomalyCLIP is fine-tuned on medical image data (ColonDB), its performance varies depending on the dataset.  
> (performance degradation in COVID-19, ISIC, TN3K)

</details>



### 3.3. Ablation study

<details><summary>Module ablation</summary>
<img width="55%" alt="Table4_module" src="https://github.com/user-attachments/assets/71ee5df8-9557-491c-9927-e2f62c7ee39d">

$$T_1$$: DPAM  
$$T_2$$: object-agnostic text prompts  
$$T_3$$: learnable tokens in text encoders  
$$T_4$$: multi-layer visual encoder features  

</details>


<details><summary>Context optimization</summary>
<img width="55%" alt="Table5_context" src="https://github.com/user-attachments/assets/9142cdf8-e34e-48c4-978d-ede0c9761c7e">
</details>


<details><summary>DPAM strategy ablation</summary>

<img width="100%" alt="Figure6_dpam" src="https://github.com/user-attachments/assets/68780eb8-63ad-4823-bab1-049ef4d61ff3">

Compared to V-V self-attention,  
Q-Q self-attention performas similar on pixel-level anomalies, but degrades on image-level anomalies.  
K-K self-attention performas similar on image-level anomalies, but degrades on pixel-level anomalies.  

Why V-V self-attention is bettern than Q-Q or K-K self-attention?  
Since Q-K consists of Q and K, Q-Q and K-K still produce large attention score on specific tokens.  
In contrast to Q-Q and K-K, V-V does not participate in computing the Q-K, reducing the unexpected bias of specific tokens.  

</details>