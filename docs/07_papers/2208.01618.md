---
title: "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"
layout: default
parent: Papers
---

# An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion

> **Accept info**: ICLR 2023 Spotlight  
> **Authors**: Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or  
> **Affiliation**: Tel-Aviv University, NVIDIA  
> **Links**: [arXiv](https://arxiv.org/abs/2208.01618), [OpenReview](https://openreview.net/forum?id=NAQvF08TcyG), [project page](https://textual-inversion.github.io/), [GitHub](https://github.com/rinongal/textual_inversion)  
> **Task**: personalized text-to-image generation  
> **TLDR**: Personalized text-to-image generation via optimizing only a single word embedding.  



## 1. Intuition & Motivation

- Goal: language-guided generation of new, user-specific concepts.  
  (ex. text-guided personalized generation, style transfer, concept compositions, bias reduction)  


<img width="100%" alt="Figure3" src="https://github.com/user-attachments/assets/d5290162-8311-43b9-9efb-9b928da9853f">

Recently, large-scale text-to-image models have demonstrated an unprecedented capability to reason over natural language descriptions.  
However, generating a desired target, such as user-specific concept, through text is quite difficult.  
(see `Figure 3`)

To overcome this challenge, it is natural to train the T2I model to learn new concepts.  
The three most common approaches are:  
1. *Re-training* the model with an expanded dataset, which is prohibitively expensive.  
2. *Fine-tuning* on a few examples, which typically leads to catastrophic forgetting.  
3. Training an *adapter*, though previous works face difficulties, such as accessing newly learned concepts.  

Since training the T2I model has several limitations, authors frame the task as an inversion, inverting the concepts into new pseudo-words within the textual embedding space of a pre-trained text-to-image model.  



## 2. Textual Inversion
<img width="100%" alt="Figure2" src="https://github.com/user-attachments/assets/98a95ac7-236b-4f2f-ac36-96737d1775d3">

### 2.1. Approach overview

- Goal: find pseudo-words that encode new, user-specified concepts  
- Core method: find pseudo-words through a visual reconstruction objective  


### 2.2. Objective

$$v_* = \textrm{argmin}_{v} \mathbb{E}_{z \sim \mathcal{E}(x), y, \epsilon \sim \mathcal{N}(0,1), t} 
\left[ \left\| \epsilon - \epsilon_{\theta}(z_t, t, c_{\theta}(y)) \right\|_2^2 \right]$$

Find $$v_*$$ through direct optimization, by minimizing the LDM loss.  
Re-using the same training scheme as the original LDM model motivates the learned embedding to capture fine visual details unique to the concept.  



## 3. Experiments
### 3.1. Qualitative results

> Results are partially curated.  
> For each prompt, generate 16 candidates and manually select the best result.  

<details><summary>Text-guided synthesis (Figure 4)</summary>
<img width="100%" alt="Figure4" src="https://github.com/user-attachments/assets/09990a19-94b1-45f2-8332-c32e33d0c746">
</details>

<details><summary>Style transfer (Figure 6)</summary>
<img width="100%" alt="Figure6" src="https://github.com/user-attachments/assets/3bfea04b-2af0-4bda-9ab0-56415d49d1ff">
</details>

<details><summary>Concept compositions (Figure 7)</summary>
<img width="100%" alt="Figure7" src="https://github.com/user-attachments/assets/84ab6e07-bc23-485b-857b-17fb24740c35">
</details>

<details><summary>Bias reduction (Figure 8)</summary>
<img width="100%" alt="Figure8" src="https://github.com/user-attachments/assets/1c7f39bb-40d6-45cf-a5b6-68aa523fefe4">
</details>

<details><summary>Downstream applications (Figure 9)</summary>
<img width="100%" alt="Figure9" src="https://github.com/user-attachments/assets/c494e749-eaa0-41e1-b172-64a927217b1a">
</details>


### 3.2. Quantitative analysis

- Metrics  
  Reconstruction (ability to replicate the target concept): CLIP-I, user study  
  Editability (ability to modify the concepts using textual prompts): CLIP-T, user study  
- Evaluation  
  Generate 64 samples using 50 DDIM steps per prompt  
- Implementation details  
  Use LDM  
  5,000 optimization steps  
  Word embeddings were initialized with the embeddings of a single-word coarse descriptor of the object  
- Ablations  
  Inversion method  
  Traing dataset (training set size, training image diversity, training prompts)  


### 3.3. Limitations

1. Typical failure cases: difficult relational prompts  
2. Learning a single concept requires roughly two hours.  
3. Since CLIP is less sensitive to shape-preservation, CLIP-I is not a reliable metric.  
4. Textual Inversion may still struggle with learning precise shapes, instead incorporating the semantic essence of a concept.  
5. In contrast to the baseline LDM model, inverted Stable Diffusion embeddings tend to dominate the prompt and become more difficult to integrate into new, simple prompts.  