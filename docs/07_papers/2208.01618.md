---
title: "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"
layout: default
parent: Papers
---

# An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion

> **Accept info**: ICLR 2023 Spotlight  
> **Authors**: Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or  
> **Affiliation**: Tel-Aviv University, NVIDIA  
> **Links**: [arXiv](https://arxiv.org/abs/2208.01618), [OpenReview](https://openreview.net/forum?id=NAQvF08TcyG), [project page](https://textual-inversion.github.io/), [GitHub](https://github.com/rinongal/textual_inversion)  
> **Task**: personalized text-to-image generation  
> **TLDR**: Personalized text-to-image generation via optimizing only a single word embedding.  



## 1. Intuition & Motivation

- Goal: language-guided generation of new, user-specific concepts.  
  (ex. text-guided personalized generation, style transfer, concept compositions, bias reduction)  


<img width="100%" alt="Figure3" src="https://github.com/user-attachments/assets/d5290162-8311-43b9-9efb-9b928da9853f">

Recently, large-scale text-to-image models have demonstrated an unprecedented capability to reason over natural language descriptions.  
However, generating a desired target, such as user-specific concept, through text is quite difficult.  
(see `Figure 3`)

To overcome this challenge, it is natural to train the T2I model to learn new concepts.  
The three most common approaches are:  
1. *Re-training* the model with an expanded dataset, which is prohibitively expensive.  
2. *Fine-tuning* on a few examples, which typically leads to catastrophic forgetting.  
3. Training an *adapter*, though previous works face difficulties, such as accessing newly learned concepts.  

Since training the T2I model has several limitations, authors frame the task as an inversion, inverting the concepts into new pseudo-words within the textual embedding space of a pre-trained text-to-image model.  



## 2. Textual Inversion
<img width="100%" alt="Figure2" src="https://github.com/user-attachments/assets/98a95ac7-236b-4f2f-ac36-96737d1775d3">

### 2.1. Approach overview

- Goal: find pseudo-words that encode new, user-specified concepts  
- Core method: find pseudo-words through a visual reconstruction objective  


### 2.2. Objective

$$v_* = \textrm{argmin}_{v} \mathbb{E}_{z \sim \mathcal{E}(x), y, \epsilon \sim \mathcal{N}(0,1), t} 
\left[ \left\| \epsilon - \epsilon_{\theta}(z_t, t, c_{\theta}(y)) \right\|_2^2 \right]$$

Find $$v_*$$ through direct optimization, by minimizing the LDM loss.  
Re-using the same training scheme as the original LDM model motivates the learned embedding to capture fine visual details unique to the concept.  



## 3. Experiments
### 3.1. Qualitative results

> Results are partially curated.  
> For each prompt, generate 16 candidates and manually select the best result.  

<details><summary>Text-guided synthesis</summary>
<img width="100%" alt="Figure4" src="https://github.com/user-attachments/assets/09990a19-94b1-45f2-8332-c32e33d0c746">
</details>


<details><summary>Style transfer</summary>
<img width="100%" alt="Figure6" src="https://github.com/user-attachments/assets/3bfea04b-2af0-4bda-9ab0-56415d49d1ff">
</details>

<details><summary>Concept compositions</summary>
<img width="100%" alt="Figure7" src="https://github.com/user-attachments/assets/84ab6e07-bc23-485b-857b-17fb24740c35">
</details>

<details><summary>Bias reduction</summary>
<img width="100%" alt="Figure8" src="https://github.com/user-attachments/assets/1c7f39bb-40d6-45cf-a5b6-68aa523fefe4">

`Figure 8`  
Utilize a small, curated dataset in order to learn a new fairer word for a biased concept.  

</details>

<details><summary>Downstream applications</summary>
<img width="100%" alt="Figure9" src="https://github.com/user-attachments/assets/c494e749-eaa0-41e1-b172-64a927217b1a">

`Figure 9`  
Pseudo-words can be used in downstream models that build on the same initial LDM model.  

</details>


### 3.2. Quantitative analysis

- Metrics  
  reconstruction (ability to replicate the target concept): CLIP-I, user study  
  editability (ability to modify the concepts using textual prompts): CLIP-T, user study  

- Evaluation  
  generate 64 samples using 50 DDIM steps per prompt  

- Implementation details  
  5,000 optimization steps  
  word embeddings were initialized with the embeddings of a single-word coarse descriptor of the object  


<details><summary>Results</summary>
<img width="100%" alt="Figure10" src="https://github.com/user-attachments/assets/b816c7e7-d655-43be-a9c9-5a518a40bb10">

- Ablation  
  Extended latent spaces: use multi-vector, instead of single.  
  Progressive extensions: progressively train multi-vector (focus core details first, and then leverage additinoal vector to capture finer details).  
  Regularization: regularize to keep the learned embedding close to existing words (minimize L2 distance of the learned embedding to the embedding of a coarse descriptor of the object).  
  Per-image tokens: train additional per-image tokens (shared token encodes share information, while per-image token relegates per-image details).  
  Human captions: use human captions instead of learned code.  
  Reference setups: Image Only (training set images), Prompt Only (generated images with text prompt without personalized concept).  

`Figure 10 (a)` - 4 interesting observations  

1. Semantic quality of Textual Inversion is comparable to Image Only.  
2. Single-word method achieves comparable reconstruction quality, and considerably improved editability over all multi-word baselines.  
  using single pseudo-word can capture new concepts with a high degree of accuracy  
3. Distortion-editability trade-off curve.
  embeddings that lie closer to the true word distribution can be more easily modifier, but fail to capture the details of the target  
  deviating far from the word distribution enables improved reconstruction at the cost of severly diminished editing capabilities  
4. Use of human descriptions not only fails to capture their likeness, but also leads to diminished editability.  

> `Figure 10 (b)`  
> User-study results align with the CLIP-based metrics and demonstrate a similar reconstruction-editability tradeoff.  

</details>


<details><summary>Effect of training set size</summary>
<img width="65%" alt="Figure12" src="https://github.com/user-attachments/assets/bdb4aef3-102a-46d5-b999-5b6f01408cfa">

`Figure 12` - effect of the concept's training set size  
Using additional images lead to optimized embeddings which reside farther away from real word embeddings, harming editability.  

</details>



### 3.3. Limitations

<details><summary>Figure 15</summary>
<img width="100%" alt="Figure15" src="https://github.com/user-attachments/assets/901d8b32-d93f-4794-a0dd-a7759719fcdf">
</details>

- Identity preservation: Textual Inversion may still struggle with learning precise shapes, instead incorporating the semantic essence of a concept.  
- Lengthy optimization times: learning a single concept requires roughly two hours.  
- Metric: since CLIP is less sensitive to shape-preservation, CLIP-I is not a reliable metric.  
- Typical failure cases: difficult relational prompts.  
  (ex. `Figure 15, rows 2, 5`)
