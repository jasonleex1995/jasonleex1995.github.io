---
title: "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"
layout: default
parent: Papers
---

# An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion

> **Accept info**: ICLR 2023 Spotlight  
> **Authors**: Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or  
> **Affiliation**: Tel-Aviv University, NVIDIA  
> **Links**: [arXiv](https://arxiv.org/abs/2208.01618), [OpenReview](https://openreview.net/forum?id=NAQvF08TcyG), [project page](https://textual-inversion.github.io/), [GitHub](https://github.com/rinongal/textual_inversion)  
> **Task**: personalized text-to-image generation  
> **TLDR**: personalized text-to-image generation via optimizing only a single word embedding.  



## 1. Intuition & Motivation

Goal: language-guided generation of new, user-specific concepts.  
(ex. text-guided personalized generation, style transfer, concept compositions, bias reduction)  


<img width="100%" alt="Figure3" src="https://github.com/user-attachments/assets/d5290162-8311-43b9-9efb-9b928da9853f">

Recently, large-scale text-to-image models have demonstrated an unprecedented capability to reason over natural language descriptions.  
However, generating a desired target, such as user-specific concept, through text is quite difficult.  

To overcome this challenge, it is natural to train the T2I model to learn new concepts.  
The three most common approaches are:  
1. **Re-training** the model with an expanded dataset, which is prohibitively expensive.  
2. **Fine-tuning** on a few examples, which typically leads to catastrophic forgetting.  
3. Training an **adapter**, though previous works face difficulties, such as accessing newly learned concepts.  

Since training the T2I model has several limitations, it is natural to find new pseudo-words that represent new, specific concepts.  
To find such pseudo-words, authors frame the task as an inversion, inverting the concepts into new pseudo-words within the textual embedding space of a pre-trained text-to-image model.  



## 2. Textual Inversion
<img width="100%" alt="Figure2" src="https://github.com/user-attachments/assets/98a95ac7-236b-4f2f-ac36-96737d1775d3">

### 2.1. Approach overview

- Goal: find pseudo-words that encode new, user-specified concepts  
- Core method: find pseudo-words through a visual reconstruction objective  


### 2.2. Objective

$$v_* = \argmin_{v} \mathbb{E}_{z \sim \mathcal{E}(x), y, \epsilon \sim \mathcal{N}(0,1), t} 
\left[ \left\| \epsilon - \epsilon_{\theta}(z_t, t, c_{\theta}(y)) \right\|_2^2 \right]$$

Find $$v_*$$ through direct optimization, by minimizing the LDM loss.  
Re-using the same training scheme as the original LDM model motivates the learned embedding to capture fine visual details unique to the concept.  



## 3. Experiments
### 3.1. Qualitative results

> Results are partially curated.  
> For each prompt, generate 16 candidates and manually select the best result.  

<details><summary>Text-guided synthesis</summary>
<img width="100%" alt="Figure4" src="https://github.com/user-attachments/assets/09990a19-94b1-45f2-8332-c32e33d0c746">
</details>


<details><summary>Style transfer</summary>
<img width="100%" alt="Figure6" src="https://github.com/user-attachments/assets/3bfea04b-2af0-4bda-9ab0-56415d49d1ff">
</details>

<details><summary>Concept compositions</summary>
<img width="100%" alt="Figure7" src="https://github.com/user-attachments/assets/84ab6e07-bc23-485b-857b-17fb24740c35">
</details>

<details><summary>Bias reduction</summary>
<img width="100%" alt="Figure8" src="https://github.com/user-attachments/assets/1c7f39bb-40d6-45cf-a5b6-68aa523fefe4">

> Utilize a small, curated dataset in order to learn a new fairer word for a biased concept.  

</details>

<details><summary>Downstream applications</summary>
<img width="100%" alt="Figure9" src="https://github.com/user-attachments/assets/c494e749-eaa0-41e1-b172-64a927217b1a">

> Pseudo-words can be used in downstream models that build on the same initial LDM model.  

</details>


### 3.2. Quantitative analysis

> - Metrics  
>   reconstruction (ability to replicate the target concept): CLIP-I, user study  
>   editability (ability to modify the concepts using textual prompts): CLIP-T, user study  
> 
> - Evaluation  
>   generate 64 samples using 50 DDIM steps per prompt  
> 
> - Implementation details  
>   5,000 optimization steps  
>   word embeddings were initialized with the embeddings of a single-word coarse descriptor of the object  


<details><summary>Results</summary>
<img width="100%" alt="Figure10" src="https://github.com/user-attachments/assets/b816c7e7-d655-43be-a9c9-5a518a40bb10">

> Ablation  
> Extended latent spaces: use multi-vector, instead of single.  
> Progressive extensions: progressively train multi-vector (focus core details first, and then leverage additinoal vector to capture finer details).  
> Regularization: regularize to keep the learned embedding close to existing words (minimize L2 distance of the learned embedding to the embedding of a coarse descriptor of the object).  
> Per-image tokens: train additional per-image tokens (shared token encodes share information, while per-image token relegates per-image details).  
> Human captions: use human captions instead of learned code.  
> Reference setups: Image Only (training set images), Prompt Only (generated images with text prompt without personalized concept).  

> `Figure 10 (a)` - 4 interesting observations  
> 1. Semantic quality of Textual Inversion is comparable to Image Only.  
> 2. Single-word method achieves comparable reconstruction quality, and considerably improved editability over all multi-word baselines.  
>   using single pseudo-word can capture new concepts with a high degree of accuracy  
> 3. Distortion-editability trade-off curve.
>   embeddings that lie closer to the true word distribution can be more easily modifier, but fail to capture the details of the target  
>   deviating far from the word distribution enables improved reconstruction at the cost of severly diminished editing capabilities  
> 4. Use of human descriptions not only fails to capture their likeness, but also leads to diminished editability.  

> `Figure 10 (b)`  
> User-study results align with the CLIP-based metrics and demonstrate a similar reconstruction-editability tradeoff.  

</details>


<details><summary>Effect of training set size</summary>
<img width="65%" alt="Figure12" src="https://github.com/user-attachments/assets/bdb4aef3-102a-46d5-b999-5b6f01408cfa">

> `Figure 12` - effect of the concept's training set size  
> Using additional images lead to optimized embeddings which reside farther away from real word embeddings, harming editability.  

</details>



### 3.3. Limitations

<details><summary>Figure 15</summary>
<img width="100%" alt="Figure15" src="https://github.com/user-attachments/assets/901d8b32-d93f-4794-a0dd-a7759719fcdf">
</details>

- Identity preservation: Textual Inversion may still struggle with learning precise shapes, instead incorporating the semantic essence of a concept.  
- Lengthy optimization times: learning a single concept requires roughly two hours.  
- Metric: since CLIP is less sensitive to shape-preservation, CLIP-I is not a reliable metric.  
- Typical failure cases: difficult relational prompts.  
  (ex. `Figure 15, rows 2, 5`)
