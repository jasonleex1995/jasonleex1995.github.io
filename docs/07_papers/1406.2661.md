---
title: "Generative Adversarial Networks"
layout: default
parent: Papers
---


# Generative Adversarial Networks

> **Accept info**: NIPS 2014  
> **Authors**: Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio  
> **Affiliation**: Departement d’informatique et de recherche operationnelle Universite de Montreal  
> **Links**: [arXiv](https://arxiv.org/abs/1406.2661), [OpenReview](https://papers.nips.cc/paper_files/paper/2014/hash/f033ed80deb0234979a61f95710dbe25-Abstract.html), [GitHub](https://github.com/goodfeli/adversarial)  
> **TLDR**: Train generative models through adversarial training, without requiring explicit likelihood estimation.  



## 1. Intuition & Motivation

Problem: Training deep generative models is challenging, because maximum likelihood estimation typically requires approximating intractable probabilistic computations.  

Intuition: Deep learning has achieved remarkable success in training discriminative models.  
→ Instead of directly maximizing the likelihood, train a discriminative model to determine whether a sample is from the generator's distribution or the data distribution?  



## 2. Adversarial Nets
### 2.1. Approach Overview

$$\underset{G}{\mathrm{min}} \ \underset{D}{\mathrm{max}} \ V(D, G) = \mathbb{E}_{x \sim p_{\mathrm{data}}(x)} [\mathrm{log} D(x)] + \mathbb{E}_{z \sim p_{z}(z)} [\mathrm{log}(1 - D(G(z)))]$$  

D: discriminator  
G: generator  

Train D to maximize the probability of assigning the correct label to both training examples and samples from G.  
Train G to minimize the $$\mathrm{log}(1 - D(G(z)))$$, which encourages G to generate data that D cannot distinguish from real samples.   


### 2.2. Theoretical Result 1: Global Optimality of $$p_g = p_{\mathrm{data}}$$

$$V(G, D) = \mathbb{E}_{x \sim p_{\mathrm{data}}} [\mathrm{log} D(x)] + \mathbb{E}_{z \sim p_{z}} [\mathrm{log}(1 - D(G(z)))]$$  
$$= \mathbb{E}_{x \sim p_{\mathrm{data}}} [\mathrm{log} D(x)] + \mathbb{E}_{x \sim p_{g}} [\mathrm{log}(1 - D(x))]$$  
$$= \int p_{\mathrm{data}}(x) \ \mathrm{log} (D(x)) \ dx + \int p_{g}(x) \ \mathrm{log} (1 - D(x)) \ dx$$  
$$= \int p_{\mathrm{data}}(x) \ \mathrm{log} (D(x)) + p_{g}(x) \ \mathrm{log} (1 - D(x)) \ dx$$  

Since $$0 \leq D(x) \leq 1$$, $$V(G, D)$$ achieves maximum at $$D_{G}^{*}(x) = \frac{p_{\mathrm{data}}(x)}{p_{\mathrm{data}}(x) + p_g(x)}$$, where $$D_{G}^{*}(x)$$ is the optimial discriminator.  

$$C(G) = \underset{D}{\mathrm{max}} \ V(G, D)$$  
$$= \mathbb{E}_{x \sim p_{\mathrm{data}}} [\mathrm{log} D_{G}^{*}(x)] + \mathbb{E}_{x \sim p_{g}} [\mathrm{log}(1 - D_{G}^{*}(x))]$$  
$$= \mathbb{E}_{x \sim p_{\mathrm{data}}} [\mathrm{log} \frac{p_{\mathrm{data}}(x)}{p_{\mathrm{data}}(x) + p_g(x)}] + \mathbb{E}_{x \sim p_{g}} [\mathrm{log}\frac{p_{g}(x)}{p_{\mathrm{data}}(x) + p_g(x)}]$$  
$$= \mathbb{E}_{x \sim p_{\mathrm{data}}} [\mathrm{log} \frac{p_{\mathrm{data}}(x)}{(p_{\mathrm{data}}(x) + p_g(x))/2}] + \mathbb{E}_{x \sim p_{g}} [\mathrm{log}\frac{p_{g}(x)}{(p_{\mathrm{data}}(x) + p_g(x))/2}] - \mathrm{log} 4$$  
$$= KL \left ( p_{\mathrm{data}}(x) \parallel \frac{p_{\mathrm{data}}(x) + p_g(x)}{2} \right ) + KL \left ( p_{g}(x) \parallel \frac{p_{\mathrm{data}}(x) + p_g(x)}{2} \right ) - \mathrm{log} 4$$  
$$= 2 \cdot JSD \left ( p_{\mathrm{data}}(x) \parallel p_{g}(x) \right ) - \mathrm{log} 4$$  

The global minimum of $$C(G)$$ exists, and is only achieved if and only if $$p_g = p_{\mathrm{data}}$$.  


### 2.3. Adversarial Nets in Practice
<img width="100%" alt="algorithm 1" src="https://github.com/user-attachments/assets/748c7dab-82e4-4595-a814-c9777df4a63a">

In practice, we must implement the two-player minmax game using an iterative, numerical approach.  
Optimizing $$D$$ to completion in the inner loop of training is computationally prohibitive, and on finite datasets would result in overfitting.  
Instead, we alternate between $$k$$ steps of optimizing $$D$$ and one step of optimizing $$G$$.  
This results in $$D$$ being maintained near its optimal solution, so long as $$G$$ changes slowly enough.  

Early in learning, when $$G$$ is poor, $$D$$ can reject samples with high confidence because they are clearly different from the training data.  
As a result, $$\mathrm{log}(1 - D(G(z)))$$ saturates and may not provide sufficient gradient for $$G$$ to learn well.  
To address this, instead of training $$G$$ to minimize $$\mathrm{log}(1 - D(G(z)))$$, train $$G$$ to maximize $$\mathrm{log} D(G(z))$$ instead.  
This results in the same fixed point of the dynamics of $$G$$ and $$D$$, but provides much stronger gradients early in learning. 


### 2.4. Theoretical Result 2: Convergence of Algorithm 1

$$V(G, D) = \mathbb{E}_{x \sim p_{\mathrm{data}}} [\mathrm{log} D(x)] + \mathbb{E}_{z \sim p_{z}} [\mathrm{log}(1 - D(G(z)))]$$  
$$= \mathbb{E}_{x \sim p_{\mathrm{data}}} [\mathrm{log} D(x)] + \mathbb{E}_{x \sim p_{g}} [\mathrm{log}(1 - D(x))]$$  

Define $$U(p_g, D) = \mathbb{E}_{x \sim p_{\mathrm{data}}} [\mathrm{log} D(x)] + \mathbb{E}_{x \sim p_{g}} [\mathrm{log}(1 - D(x))]$$, then $$V(G, D) = U(p_g, D)$$.  

$$U(p_g, D) = \mathbb{E}_{x \sim p_{\mathrm{data}}} [\mathrm{log} D(x)] + \mathbb{E}_{x \sim p_{g}} [\mathrm{log}(1 - D(x))]$$  
$$= \int p_{\mathrm{data}}(x) \ \mathrm{log} (D(x)) + p_{g}(x) \ \mathrm{log} (1 - D(x)) \ dx$$  
→ $$U(p_g, D)$$ is convex in $$p_g$$.  

$$C(G) = \underset{D}{\mathrm{max}} \ V(G, D) = \underset{D}{\mathrm{sup}} \ U(p_g, D)$$  
→ since $$U(p_g, D)$$ is convex in $$p_g$$, $$\underset{D}{\mathrm{sup}} \ U(p_g, D)$$ is also convex in $$p_g$$.  

Training $$D$$ for $$k$$ steps → assume $$D$$ is optimum given $$G$$ → $$\underset{D}{\mathrm{sup}} \ U(p_g, D)$$  
Gradient of $$\underset{D}{\mathrm{sup}} \ U(p_g, D)$$ w.r.t. $$G$$ = Gradient of $$C(G)$$ w.r.t. $$G$$  

Convex and use gradient → converges to global optima  

For rigorous proof, use subderivatives of supremum of convex functions.  
(if $$f$$ is convex but possibly non-differentiable, then gradient descent using any subgradient will converge to a global minimum under mild conditions)  