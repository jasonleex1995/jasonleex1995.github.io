---
title: "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"
layout: default
parent: Papers
---


# DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation

> **Accept info**: CVPR 2023 Award Candidate  
> **Authors**: Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman    
> **Affiliation**: Google Research, Boston University  
> **Links**: [arXiv](https://arxiv.org/abs/2208.12242), [project page](https://dreambooth.github.io/), [GitHub](https://github.com/google/dreambooth)  
> **Task**: subject-driven image generation  
> **TLDR**: personalization of T2I diffusion models via fine-tuning with rare tokens and class-specific prior preservation loss.  


## 1. Intuition & Motivation

Goal: generate novel photorealistic images of the subject, given only a few (typically 3-5) casually captured images of a specific subject, without any textual description.  
(ex. recontextualization, accessorization, property modification)


<img width="100%" alt="Figure23_comparison" src="https://github.com/user-attachments/assets/22884694-73ab-45ef-8e74-84d7925bba47">

Recently developed large text-to-image diffusion models have shown unprecedented capabilities, by enabling high-quality and diverse synthesis of images based on a text prompt written in natural language.  
While the sysnthesis capabilities of these models are unprecedented, they lack the ability to mimic the appearance of subjects in a given reference set, and synthesize novel renditions of the same subjects in different contexts.  
Even the most detailed textual description of an object may yield instances with different appearances.  
Thus, it is natural to infer that the expressiveness of their output domain is limited.  

To bind new words with specific subjects, the most simple approach is to fine-tune a pre-trained, diffusion-based text-to-image models.  



## 2. DreamBooth
<img width="60%" alt="Figure3_method" src="https://github.com/user-attachments/assets/d038246c-0ce2-4a88-bbf9-9f720f798653">


### 2.1. Approach overview

- Goal: implant a new (unique identifier, subject) pair into the diffusion model's dictionary  
- Core method: fine-tune pre-trained T2I diffusion models  
- Prompt design: `a [identifier] [class noun]`  
  `identifier`: unique identifier linked to the subject  
  `class noun`: coarse class descriptor of the subject  
- Class-specific prior preservation loss  
  to mitigate language drift and reduced output diversity  


### 2.2. Prompt design

Prompt: `a [identifier] [class noun]`  

To bypass the overhead of writing detailed image descriptions, use simple prompt.  
To leverage the model's prior of the specific class and entangle it with the embedding of the subject's unique identifier, use coarse class descriptor of the subject `[class noun]`.  

How should we design rare-token identifiers?  
Existing english words already have their original meaning.  
Thus, an identifier should have a weak prior in both the language model and the diffusion model.  

Paper's approach: rare-token lookup in the vocabulary and obtain a sequence of rare token identifiers.  
The sequence can be of variable length $$k$$, and relatively short sequences of $$k = \left\{ 1, 2, 3\right\}$$ work well.  


### 2.3. Class-specific prior preservation loss

From experience, fine-tuning all layers of the model achieves the best subject fidelity.  
However, this causes two problems.  

1. Language drift  
  model slowly forgets how to generate the subject's class  
2. Reduced output diversity  
  fine-tuning on a small set of images reduces the amount of variability  
  (ex. pose, view)   

$$\mathbb{E}_{\mathbf{x}, \mathbf{c}, \mathbf{\epsilon}, \mathbf{\epsilon}', t} 
\left[ w_t \left\| \hat{\mathbf{x}}_{\theta} (\alpha_t \mathbf{x} + \sigma_t \mathbf{\epsilon}, \mathbf{c}) - \mathbf{x} \right\|_2^2 
+ \lambda w_{t'} \left\| \hat{\mathbf{x}}_{\theta} (\alpha_{t'} \mathbf{x}_{pr} + \sigma_{t'} \mathbf{\epsilon}', \mathbf{c}_{pr}) - \mathbf{x}_{pr} \right\|_2^2 \right]$$  

To mitigate two issues, authors propose class-specific prior preservation loss.  
(second term of above equation)  
The core idea of this loss is to fine-tune the model with images of the subject's class.  
By fine-tuning the model with its own generated samples, the fine-tuned models are able to retain the original prior of the subject's class, which prevent both lagnauge drift and reduced output diversity. 



## 3. Experiments
### 3.1. Experiment setup

- Dataset  
  total 30 subjects, 25 prompts per subject  
  objects (21) - recontextualization (20), property modification (5)  
  live subjects/pets (9) - recontextualization (10), accessorization (10), property modification (5)  
- Evaluation  
  generate 4 images per subject and per prompt, total 3,000 images  
- Metrics  
  subject fidelity: CLIP-I, DINO, user study  
  prompt fidelity: CLIP-T, user study  
- Implementation details  
  train ~ 1,000 iterations  
  use relative weight $$\lambda = 1$$ for prior preservation loss  
  use ViT-S/16 DINO for DINO metric  
  for Stable Diffusion, train U-Net (and possibly the text encoder)  
  generate ~ 1,000 images with text prompt `a [class noun]` for class-specific prior preservation loss  


### 3.2. Main results

<details><summary>Table 1</summary>
<img width="55%" alt="Table1_result" src="https://github.com/user-attachments/assets/cc5c095a-530c-4707-ba47-d1f109ad24b2">

> `Table 1`  
> DreamBooth (Imagen) > DreamBooth (Stable Diffusion) > Textual Inversion (Stable Diffusion) 

</details>

 
<details><summary>Table 2</summary>
<img width="55%" alt="Table2_user" src="https://github.com/user-attachments/assets/913bc782-4e05-49a3-834e-0140becb7949">

> `Table 2`  
> Users overwhelmingly prefer DreamBooth for both subject fidelity and prompt fidelity.  

</details>


<details><summary>Figure 4</summary>
<img width="55%" alt="Figure4_compare" src="https://github.com/user-attachments/assets/1f078c67-5453-4f6f-bd84-3f6ba28103a8">

> `Figure 4`  
> DreamBooth better preserves subjec identity, and is more faithful to prompts.  

</details>



### 3.3. Ablation study


<details><summary>Prior preservation loss</summary>

> Prior preservation loss (PPL)  
> 
> Prior preservation metric (PRES): DINO between generated images of subject's class and real images of subject.  
> (high PRES means random class images and subject images are similar, indicating collapse of the prior)  
> 
> Diversity metric (DIV): LPIPS between generated images of same subject with same prompt.  
> (high DIV means generated images are similar)  

<img width="100%" alt="Figure19_PPL" src="https://github.com/user-attachments/assets/58afd9fb-8707-464f-afaf-208c71b517f2">
<img width="55%" alt="Table3_PPL" src="https://github.com/user-attachments/assets/59a5387d-f6ed-4305-ba3f-ae1c846ae59b">

> `Table 3`  
> PPL substantially counteracts language drift and helps retain the ability to generate diverse images of the prior class.  
> PPL achieves higher diversity with slightly diminished subject fidelity.  

</details>


<details><summary>Class-prior</summary>
<img width="55%" alt="Table4_class" src="https://github.com/user-attachments/assets/e4ecc57d-d449-41a9-b809-f67a84ce325f">

> `Table 4`  
> Using correct subject's class is able to faithfully fit to the subject, take advantage of the class prior, allowing to generate subjects in various contexts.  

</details>


<details><summary>Effect of training images</summary>
<img width="100%" alt="Figure20" src="https://github.com/user-attachments/assets/149cddd3-e47f-4d36-82e7-a7f0adc9bef4">

> `Figure 20`  
> Common subjects such as Corgi dog are able to accurately capture the appearance using only two images.  
> However, rare subjects such as selected backpack need more samples to accurately preserve the subject and to recontextualize it to diverse settings.  

<img width="55%" alt="Table5" src="https://github.com/user-attachments/assets/d0cb629b-203a-4e4d-b8a0-db90d7cdfab7">
<img width="55%" alt="Table6" src="https://github.com/user-attachments/assets/c930d653-6db2-4cae-90af-a31b7fdfde68">

> `Table 5, 6`  
> Optimal amount of input images is 4, but this number can vary depending on the subject.  
> Authors recommend to use 3 ~ 5 images for model personalization.  

</details>



### 3.4. Applications

<details><summary>Recontextualization</summary>
<img width="100%" alt="Figure7" src="https://github.com/user-attachments/assets/535ac27c-779e-4201-ba7d-21d59675349a">
</details>


<details><summary>Art renditions</summary>
<img width="100%" alt="Figure13" src="https://github.com/user-attachments/assets/62551c34-6b74-473d-abae-e83965d5e911">
</details>


<details><summary>Expression manipulation</summary>
<img width="100%" alt="Figure14" src="https://github.com/user-attachments/assets/3794ae2c-a1c3-4eb7-966d-8b908a43e713">
</details>


<details><summary>Novel view synthesis</summary>
<img width="100%" alt="Figure15" src="https://github.com/user-attachments/assets/65530fd2-e757-4d97-bfc8-4c08970a7ebd">
</details>


<details><summary>Accessorization</summary>
<img width="100%" alt="Figure16" src="https://github.com/user-attachments/assets/e32bfb44-10c4-4688-a868-600f492e9835">
</details>


<details><summary>Property modification</summary>
<img width="100%" alt="Figure17" src="https://github.com/user-attachments/assets/f4d28c50-e18e-4e93-9044-e4fedf1fa0f5">
</details>


<details><summary>Comic book generation</summary>
<img width="100%" alt="Figure18" src="https://github.com/user-attachments/assets/a6e9a7b1-52e5-4e33-b602-94f12932638e">
</details>


### 3.5. Limitations

<details><summary>Figure 9</summary>
<img width="55%" alt="Figure9_limit" src="https://github.com/user-attachments/assets/6417b372-b701-47c5-b9af-5e75fd4b0949">
</details>


1. Incorrect context synthesis (`Figure 9 (a)`)  
  possible reasons: weak prior of the context, difficulty in generating both the subject and specified concept together  
2. Context-appearance entanglement (`Figure 9 (b)`)  
3. Overfitting to real images (`Figure 9 (c)`)  
  overfitting observed when the prompt is similar to the original setting  
4. Dependency of base model  
  for rare subjects, the model is unable to support as many subject variations  
  variability in the fidelity of the subject  
  hallucinated subject features  



## 4. Appendix
### 4.1. Subject fidelity metrics
<img width="100%" alt="Figure11" src="https://github.com/user-attachments/assets/4cbfceaa-cafc-4ac0-9097-2c55475cb8a4">

CLIP is not constructed to distinguish between different subjects that could have highly similar text descriptions.  
However, DINO is trained in a self-supervised manner to distinguish different images from each other modulo data augmentations.  
Thus, DINO metric is superior than CLIP-I in terms of subject fidelity.  

In order to quantitatively test this, authors compute correlations between DINO/CLIP-I scores and normalized human preference scores.  
Pearson correlation coefficient: DINO (0.32) > CLIP-I (0.27)  